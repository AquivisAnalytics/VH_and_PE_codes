---
title: "KSVM"
output: ''
---

Implements kernelized Support Vector Machines (KSVM).

```{r}
library(class)
library(car)
library(tables)
library(RcmdrMisc)
library(dplyr)
library(MASS)
library(klaR)
library(splitstackshape)
library(kernlab)
library(rpart)
library(caret)
library(randomForest)
library(ROSE)
library(DMwR2)
library(Metrics)
```


### Parameters

Parameters of the script:

*   **csv**: "factors_2_numbers_del.csv", "factors_2_numbers_del_cont.csv" or "onehot_del_cont.csv". CSV containing the data to work with. 

*   **data**: "continuous", "categorized" or "onehot".

*   **scaled**: TRUE, FALSE. TRUE if continuous data is scaled.

*   **target_name**: "target_1", "target_6", "target_12" or "target_future".

```{r}
csv <- "factors_2_numbers_del.csv"
data <- "categorized"
target_name <- "target_1"

scaled <- FALSE
set.seed <- 433
```

### Reading the data

Reading continuous, categorized, or one hot data. Deleting some columns which values are incorrect or not useful for training the models.

```{r}
X <- read.csv(paste0('../../Data/',csv))
drop <- names(X) %in% c("mg_sacu_seguim", "nhc")
X <- subset(X, select = !drop)
```

### Factors and ordinal variables

Defining categorical variables as factors. The following vectors (categorical and ordered) contain the names of all categorical and ordinal variables.

1. For non-one-hot data:

```{r}
if (data != "onehot") {
  
  categorical <- c('sexe','data_visita_year','data_visita_month','data_visita_week','data_visita_day','procedencia','motiu_derivacio___1','motiu_derivacio___2','motiu_derivacio___3','motiu_derivacio___4','motiu_derivacio___5','motiu_derivacio___6','motiu_derivacio___7','motiu_derivacio___8','etiologia___1','etiologia___2','etiologia___3','etiologia___4','etiologia___5','etiologia___6','etiologia___7','etiologia___8','etiologia___9','etiologia___10','etiologia___11','etiologia___12','etiologia___13','antecedents___1','antecedents___2','antecedents___3','antecedents___4','antecedents___5','antecedents___6','antecedents___7','antecedents___8','antecedents___9','antecedents___10','antecedents___11','antecedents___12','antecedents___21','antecedents___13','antecedents___14','antecedents___15','antecedents___16','antecedents___17','antecedents___20','neoplasia_estat','neoplasia_qt','tipus_iqprevia','insuf_mitral_seguim','ritme_base_seguim','trastorn_conduccio_t_v_1','marca','tto_ev_tipo___1','tto_ev_tipo___2','tto_ev_tipo___3','tto_ev_tipo___4','tto_ev_tipo___5','classe_funcional_seguim','ttm_seguim___8','ttm_seguim___9','ttm_seguim___10','ttm_seguim___11','ttm_seguim___12','ttm_seguim___13','ttm_seguim___14','ttm_seguim___15','ttm_seguim___16','ttm_seguim___17','ttm_seguim___18','ttm_seguim___20','ttm_seguim___21','ttm_seguim___23','ttm_seguim___24','ttm_seguim___25','ttm_seguim___26','ttm_seguim___27','ttm_seguim___28','ttm_seguim___29','ttm_seguim___30','ttm_seguim___33','mg_diur_segui','mg_tiaz_seguim','mg_anti_seguim','mg_ieca_seguim','mg_ara2_seguim','mg_beta_seguim','mg_islgt2_seguim','mg_naco_seguim','estacio_visita','IMC','mg_diur_segui')

  ordered <- c('insuf_mitral_seguim','mg_tiaz_seguim','mg_anti_seguim','mg_ieca_seguim','mg_ara2_seguim','mg_beta_seguim','mg_islgt2_seguim','mg_naco_seguim','IMC','classe_funcional_seguim','mg_diur_segui')
}
```

2. For categorized data:

```{r}
if (data == "categorized") { 
  categorized <- c("feve_seguim", "dtdve_seguim", "tiv_seguim", "paret", "auricula_seguim", "temps_desac_seguim", 
                 "tapse_seguim", "paps_seguim", "amplada_qrs_seguim", "hb_seguim", "ferritina_seguim", "sat_transf_seguim", "ha1c_seguim", "creat_seguim", "fge_seguim", 
                 "urat_seguim", "sodi_seguim", "potassi_seguim", "cloro_seguim", "tni_seguim", "probnp_seguim", 
                 "cole_seguim", "colehdl_segui", "coleldl_segui", "trigli_seguim", "prot_seguim", "albu_seguim", 
                 "ca125_seguim", "st2_seguim", "prot_creorin_seguim", "albu_crorin_seguim",
                 "ona_e_seguim", "ona_a_seguim", "ona_e_prima_seguim")
  categorical <- c(categorical, categorized)
  ordered <- c(ordered, categorized)
}
```

3. For one-hot data:

```{r}
if (data == "onehot") {
  one_hot <- c()
  
  for (c in colnames(X)) {
    if (length(unique(X[,c])) <= 2) {
      one_hot <- c(one_hot, c)
    }
  }
  
  categorical <- c(one_hot, 'data_visita_year','data_visita_month','data_visita_week','data_visita_day','insuf_mitral_seguim','classe_funcional_seguim','mg_diur_segui','mg_tiaz_seguim','mg_anti_seguim','mg_ieca_seguim','mg_ara2_seguim','mg_beta_seguim','mg_islgt2_seguim','mg_naco_seguim','IMC')
  ordered <- c('insuf_mitral_seguim','classe_funcional_seguim','mg_diur_segui','mg_tiaz_seguim','mg_anti_seguim','mg_ieca_seguim','mg_ara2_seguim','mg_beta_seguim','mg_islgt2_seguim','mg_naco_seguim','IMC')
}
```

Defining categorical variables as "factors" and ordinal variables as "ordered". Otherwise, they would be interpreted as integers.

```{r}
for (c in categorical) {
  index_c <- which(colnames(X) == c)
  X[,index_c] <- as.factor(X[,index_c])
}

for (o in ordered) {
  index_o <- which(colnames(X) == o)
  X[,index_o] <- as.ordered(X[,index_o])
}
```

### Stratified sampling

Splitting the data into **0.7 training and 0.3 test**. Performing stratified sampling so the proportion of class members in the training and test sets is homogeneous for all target variables.

```{r}
set.seed(set.seed)
index_c <- which(colnames(X) %in% c("target_1", "target_6","target_12", "target_future"))
variables <- X[,-c(index_c)]
training_set <- as.data.frame(stratified(X, c("target_1", "target_6","target_12", "target_future"), 0.7))
test_set <- as.data.frame(setdiff(X, training_set))
```

Changing the name of the **target variable** in question to "target", and dropping other target variables.

```{r}
names(training_set)[names(training_set) == target_name] <- "target"
drop <- names(training_set) %in% c("target_1", "target_6", "target_12", "target_future")
training_set <- subset(training_set, select = !drop)

names(test_set)[names(test_set) == target_name] <- "target"
drop <- names(test_set) %in% c("target_1", "target_6", "target_12", "target_future")
test_set <- subset(test_set, select = !drop)
```

Deleting binary categorical variables containing less than three values for one of the categories. Otherwise, it would lead to computational errors.

```{r}
set.seed(set.seed)
name_columns <- c()

for ( c in colnames(training_set)) {
  if(length(unique(training_set[,c])) <= 2) {
    repeticions <- sum(training_set[,c] == unique(training_set[,c])[1])
    if (repeticions <= 2 || (nrow(training_set)-repeticions) <= 2) {
      name_columns <- c(name_columns, c)
    }
  }
}

drop <- (names(training_set) %in% name_columns)
training_set <- subset(training_set, select = !drop)
test_set <- subset(test_set, select = !drop)
```


### Data standardization

Standardizing numerical variables to force them to have 0 mean and 1 standard deviation (sd). The standardization of the test set is performed with the same mean and sd of the training set.

```{r}
set.seed(set.seed)
if (!scaled) {
  numerical <- !(colnames(training_set) %in% categorical)
  numerical[length(numerical)] <- FALSE
  normParam <- preProcess(training_set[, numerical])
  training_set_num <- predict(normParam, training_set[, numerical])
  test_set_num <- predict(normParam, test_set[, numerical])
 
  training_set[, numerical] <- data.frame(scale(training_set_num))
  test_set[, numerical] <- data.frame(test_set_num)
}
```


### Balancing method

**Oversampling**

Oversampling the minority class (1) until reaching the same number of observations as the majority class (0).

```{r}
oversampling_fun <- function(training_data) {
  set.seed(set.seed)
  training_set_over <- ovun.sample(target ~ ., data = training_data, method = 'over', N = c(sum(training_data$target == 0)*2))$data
  return(training_set_over)
}
```

**Undersampling**

Undersampling the majority class (0) until reaching the same number of observations as the minority class (1).

```{r}
undersampling_fun <- function(training_data) {
  set.seed(set.seed)
  training_set_under <- ovun.sample(target ~ ., data = training_data, method = 'under', N = sum(training_data$target == 1)*2)$data
  
  return(training_set_under)
}
```

**Both**

Combination of both oversampling and undersampling: the majority class is undersampled without replacement and the minority class is oversampled with replacement. In this case, the final total number of training rows is 1.3 times the original number of training rows.

```{r}
both_fun <- function(training_data) {
  set.seed(set.seed)
  training_set_both <- ovun.sample(target ~ ., data = training_data, method = 'both', p = .5, N = nrow(training_data)*1.3)$data
  
  return(training_set_both)
}
```

**ROSE**

Generating data synthetically providing a better estimate of original data.

```{r}
ROSE_fun <- function(training_set, test_set, o) {
  set.seed(set.seed)
  aux <- rbind(training_set, test_set)
  for (o in ordered) {
    index_o <- which(colnames(aux) == o)
    aux[,index_o] <- as.factor(as.numeric((aux[,index_o])))
  }
   
  
  training_set_ROSE <- aux[1:nrow(training_set),]
  test_set_ROSE <- aux[(nrow(training_set)+1):nrow(aux),]
  training_set_ROSE <- ROSE(target ~ ., data = training_set_ROSE)$data
  
  return(list(training_set_ROSE, test_set_ROSE))
}
```


### Some functions of use

Definition of some functions that will be used during the training and testing of the ML models.

1. KK cross-validation function: kk_crossvalidation().

2. Calculating the errors: err_fun().

#### 1. KK cross-validation

Function that performs a KK CV over the training set. The split of the training set into K folds is stratified by the target.

The validation performance at each step (at each time and fold) is stored as a new row. All performance results are stored in a data frame called error. Finally, calculates the mean of each performance column and rounds to 2 decimals. The performance metrics calculated are (1) Accuracy, (2) Sensitivity, (3) Specificity.

```{r}
kk_crossvalidation <- function(training_set, balancing = "none", times = 3, k = 3,
                               type = NULL, kernel = NULL, C = 1, degree = 1) {
  error <- c()
  set.seed(set.seed)
  for (time in 1:times) {
      folds <- createFolds(factor(training_set$target), k = k, list = TRUE)
      for (fold in folds) {
          validation_data <<- data.frame(training_set[fold, ])
          training_data <<- data.frame(training_set[-fold, ])
          
          if (balancing == "oversampling") {training_data <- oversampling_fun(training_data)}
          else if (balancing == "undersampling") {training_data <- undersampling_fun(training_data)}
          else if (balancing == "both") {training_data <- both_fun(training_data)}
          if (balancing == "ROSE") {
            ROSE_datasets <<- ROSE_fun(training_data, validation_data)
            training_data <<- data.frame(ROSE_datasets[1])
            validation_data <<- data.frame(ROSE_datasets[2])
          }
          error <- rbind(error, ksvm_fun(training_data, validation_data, type, kernel, C, degree))
      }
  }
  
  return (round(colMeans(error),2))
}
```


#### 2. Calculating the errors

Function that given a confusion matrix, extracts and returns three evaluation metrics in a data frame:

(1) Accuracy (that is, the proportion of correctly predicted classes out of all the validation data points). 
(2) Sensitivity (that is, the proportion of 1's which are correctly classified as 1's). 
(3) Specificity (that is, the proportion of 0's which are correctly classified as 0's).

```{r}
err_fun <- function(confusion_matrix) {
  set.seed(set.seed)
  if (length(confusion_matrix) == 4) {
    Accuracy <- 100*(confusion_matrix[1]+confusion_matrix[4])/sum(confusion_matrix)
    Sensitivity <- 100*(confusion_matrix[4])/(confusion_matrix[3]+confusion_matrix[4])
    Specificity <- 100*(confusion_matrix[1])/(confusion_matrix[1]+confusion_matrix[2])
  } else if (rownames(confusion_matrix) == 0) {
    Accuracy <- confusion_matrix[1]/sum(confusion_matrix)*100
    Sensitivity <- 0
    Specificity <- 100
  } else if (rownames(confusion_matrix) == 1) {
    Accuracy <- confusion_matrix[2]/sum(confusion_matrix)*100
    Sensitivity <- 100
    Specificity <- 0
  }
  
  return(cbind(Accuracy, Sensitivity, Specificity))
}
```


### Classification models

To access how KSVM models will generalize to an independent data set and flag overfitting problems, we perform 3x3 Cross-Validation (CV) to find the best hyperparameters for each model.

**Types of SVM classification to be used:**

* C-svc: C classification.

* C-bsvc: bound-constraint SVM classification.


**List of kernel hyperparameters to be defined for each type and kernel:**

* C: constant of the regularization term in the Lagrange formulation. From zero to infinity. We take 10^seq(0.1,2,0.1).

* Sigma: inverse kernel width for the Radial Basis kernel function "rbfdot". We use automatic estimation.

* Degree: for the ANOVA kernel "anovadot, we take values from 1 to 3.


**ksvm_fun()**

Function that implements the KSVM method for some given **kernel** and **type** and returns its performance evaluation (Accuracy, Sensitivity, and Specificity).

```{r}
ksvm_fun <- function(training_set, validation_set, type, kernel, C, degree) {
  set.seed(set.seed)
  my.SVM.TR <- ksvm(target ~ ., data = training_set, type = type, kernel = kernel, C = C, degree = degree)
  fit <- predict(my.SVM.TR, newdata = validation_set)
  confusion_matrix <- table(fit, validation_set$target)

  return(err_fun(confusion_matrix))
}
```


#### Rbfdot

Defining the range of values for the C hyperparameter for C-svc and C-bsvc types of classification. 

```{r}
C <- c(1, 10^seq(0.1, 2, by = 0.1))
```

**rbfdot_csvc()**

Function that performs hyperparameter tuning and stores performance results (Accuracy, Sensitivity, and Specificity) for C-svc type and rbfdot kernel. The name of its rows describes the hyperparameters used at each iteration. 

Sorts the rows in decreasing order: 0.6·Sensitivity + 0.4·Specificity.

```{r}
rbfdot_csvc <- function(training_set, balancing) {
  set.seed(set.seed)
  rbfdot_csvc <- data.frame()
  for (i in c(1:20)) {
    C_i <- C[i]
    rbfdot_csvc_k <- kk_crossvalidation(training_set = training_set, balancing = balancing, type = 'C-svc', kernel = 'rbfdot', C = C_i)
    rbfdot_csvc_k <- c(rbfdot_csvc_k,C_i)
    rbfdot_csvc <- rbind(rbfdot_csvc, rbfdot_csvc_k)
    rownames(rbfdot_csvc)[i] = paste("it =", i, "C =", C_i)
  }
  
  colnames(rbfdot_csvc) <-  c("Accuracy", "Sensitivity", "Specificity", "C")
  rbfdot_csvc <- rbfdot_csvc[with(rbfdot_csvc, order(0.6*rbfdot_csvc[,2]+0.4*rbfdot_csvc[,3], decreasing = TRUE)), ]

  return (rbfdot_csvc[1,])
}
```

**rbfdot_cbsvc()**

Function that performs hyperparameter tuning and stores performance results (Accuracy, Sensitivity, and Specificity) for C-bsvc type and rbfdot kernel. The name of its rows describes the hyperparameters used at each iteration. 

Sorts the rows in decreasing order: 0.6·Sensitivity + 0.4·Specificity.

```{r}
rbfdot_cbsvc <- function(training_set, balancing) {
  set.seed(set.seed)
  rbfdot_cbsvc <- data.frame()
  
  for (i in c(1:20)) {
    C_i <- C[i]
    rbfdot_cbsvc_k <- kk_crossvalidation(training_set = training_set, balancing = balancing, type = 'C-bsvc', kernel = 'rbfdot', C = C_i)
    rbfdot_cbsvc_k <- c(rbfdot_cbsvc_k,C_i)
    rbfdot_cbsvc <- rbind(rbfdot_cbsvc, rbfdot_cbsvc_k)
    rownames(rbfdot_cbsvc)[i] = paste("it =", i, "C =", C_i)
  }
  
  colnames(rbfdot_cbsvc) <- c("Accuracy", "Sensitivity", "Specificity", "C")
  rbfdot_cbsvc <- rbfdot_cbsvc[with(rbfdot_cbsvc, order(0.6*rbfdot_cbsvc[,2]+0.4*rbfdot_cbsvc[,3], decreasing = TRUE)), ]

  return (rbfdot_cbsvc[1,])
}
```

#### Vanilladot

**vanilladot_csvc()**

Function that performs hyperparameter tuning and stores performance results (Accuracy, Sensitivity, and Specificity) for C-svc type and vanilladot kernel. The name of its rows describes the hyperparameters used at each iteration. 

Sorts the rows in decreasing order: 0.6·Sensitivity + 0.4·Specificity.

```{r}
vanilladot_csvc <- function(training_set, balancing) {
  set.seed(set.seed)
  vanilladot_csvc <- data.frame()
  
  for (i in c(1:20)) {
    C_i <- C[i]
    vanilladot_csvc_k <- kk_crossvalidation(training_set = training_set, balancing = balancing, type = 'C-svc', kernel = 'vanilladot', C = C_i)
    vanilladot_csvc_k <- c(vanilladot_csvc_k,C_i)
    vanilladot_csvc <- rbind(vanilladot_csvc, vanilladot_csvc_k)
    rownames(vanilladot_csvc)[i] = paste("it =", i, "C =", C_i)
  }
  
  colnames(vanilladot_csvc) <-  c("Accuracy", "Sensitivity", "Specificity", "C")
  vanilladot_csvc <- vanilladot_csvc[with(vanilladot_csvc, order(0.6*vanilladot_csvc[,2]+0.4*vanilladot_csvc[,3], decreasing = TRUE)), ]
  
  return (vanilladot_csvc[1,])
}
```

#### Anovadot

**anovadot_csvc()**

Function that performs hyperparameter tuning and stores performance results (Accuracy, Sensitivity, and Specificity) for C-svc type and anovadot kernel. The name of its rows describes the hyperparameters used at each iteration. 

Sorts the rows in decreasing order: 0.6·Sensitivity + 0.4·Specificity.

```{r}
anovadot_csvc <- function(training_set, balancing) {
  set.seed(set.seed)
  anovadot_csvc <- data.frame()
  
  for (i in c(1:20)) {
      degree_i <- sample(1:3,1)  
      C_i <- C[i]
      anovadot_csvc_k <- kk_crossvalidation(training_set = training_set, balancing = balancing, type = 'C-svc', kernel = 'anovadot', C = C_i, degree = degree_i)
      anovadot_csvc_k <- c(anovadot_csvc_k,C_i,degree_i)
      anovadot_csvc <- rbind(anovadot_csvc, anovadot_csvc_k)
      rownames(anovadot_csvc)[i] = paste("it =", i, "C =", C_i,"Degree = ", degree_i)
  }
  
  colnames(anovadot_csvc) <-  c("Accuracy", "Sensitivity", "Specificity", "C", "Degree")
  anovadot_csvc <- anovadot_csvc[with(anovadot_csvc, order(0.6*anovadot_csvc[,2]+0.4*anovadot_csvc[,3], decreasing = TRUE)), ]

  return (anovadot_csvc[1,])
}
```


### Training and evaluating (cross-validation)

Calculating performance for all kernels, types, and balancing methods. The balancing method is applied to the folds of the training set dedicated to training the models at each step of the cross-validation to prevent overfitting.

```{r}
set.seed(set.seed)
csv <- "reduction"
rbfdot_csvc_over <- cbind(rbfdot_csvc(training_set, "oversampling"), "-", csv, "rbfdot", "C-svc", "over")
rbfdot_cbsvc_over <- cbind(rbfdot_cbsvc(training_set, "oversampling"), "-", csv, "rbfdot", "C-bsvc", "over")
vanilladot_csvc_over <- cbind(vanilladot_csvc(training_set, "oversampling"), "-", csv, "vanilladot", "C-svc", "over")
anovadot_csvc_over <- cbind(anovadot_csvc(training_set, "oversampling"), csv, "anovadot", "C-svc","over")

rbfdot_csvc_under <- cbind(rbfdot_csvc(training_set, "undersampling"), "-", csv, "rbfdot", "C-svc", "under")
rbfdot_cbsvc_under <- cbind(rbfdot_cbsvc(training_set, "undersampling"), "-", csv, "rbfdot", "C-bsvc", "under")
vanilladot_csvc_under <- cbind(vanilladot_csvc(training_set, "undersampling"), "-", csv, "vanilladot", "C-svc", "under")
anovadot_csvc_under <- cbind(anovadot_csvc(training_set, "undersampling"), csv, "anovadot", "C-svc", "under")

rbfdot_csvc_both <- cbind(rbfdot_csvc(training_set, "both"), "-", csv, "rbfdot", "C-svc", "both")
rbfdot_cbsvc_both <- cbind(rbfdot_cbsvc(training_set, "both"), "-", csv, "rbfdot", "C-bsvc", "both")
vanilladot_csvc_both <- cbind(vanilladot_csvc(training_set, "both"), "-", csv, "vanilladot", "C-svc", "both")
anovadot_csvc_both <- cbind(anovadot_csvc(training_set, "both"),csv, "anovadot", "C-svc", "both")

rbfdot_csvc_ROSE <- cbind(rbfdot_csvc(training_set, "ROSE"), "-", csv, "rbfdot", "C-svc", "ROSE")
rbfdot_cbsvc_ROSE <- cbind(rbfdot_cbsvc(training_set, "ROSE"), "-", csv, "rbfdot", "C-bsvc", "ROSE")
vanilladot_csvc_ROSE <- cbind(vanilladot_csvc(training_set, "ROSE"), "-", csv, "vanilladot", "C-svc", "ROSE")
anovadot_csvc_ROSE <- cbind(anovadot_csvc(training_set, "ROSE"), csv, "anovadot", "C-svc", "ROSE")
```

Giving the same names to the columns of the previous data frames.

```{r}
column_names <- c("Accuracy", "Sensitivity", "Specificity", "C", "Degree", "Data", "Kernel", "Type", "Balancing")
colnames(rbfdot_csvc_over) <- column_names
colnames(rbfdot_cbsvc_over) <- column_names
colnames(vanilladot_csvc_over) <- column_names
colnames(anovadot_csvc_over) <- column_names

colnames(rbfdot_csvc_under) <- column_names
colnames(rbfdot_cbsvc_under) <- column_names
colnames(vanilladot_csvc_under) <- column_names
colnames(anovadot_csvc_under) <- column_names
             
colnames(rbfdot_csvc_both) <- column_names
colnames(rbfdot_cbsvc_both) <- column_names
colnames(vanilladot_csvc_both) <- column_names
colnames(anovadot_csvc_both) <- column_names

colnames(rbfdot_csvc_ROSE) <- column_names
colnames(rbfdot_cbsvc_ROSE) <- column_names
colnames(vanilladot_csvc_ROSE) <- column_names
colnames(anovadot_csvc_ROSE) <- column_names
```

Concatenating performance results in a single data frame. 

```{r}
cv_result <- data.frame()

cv_result <- rbind(cv_result, rbfdot_csvc_over, rbfdot_cbsvc_over, vanilladot_csvc_over, anovadot_csvc_over, 
                   rbfdot_csvc_under, rbfdot_cbsvc_under, vanilladot_csvc_under, anovadot_csvc_under, 
                   rbfdot_csvc_both, rbfdot_cbsvc_both, vanilladot_csvc_both, anovadot_csvc_both, 
                   rbfdot_csvc_ROSE, rbfdot_cbsvc_ROSE, vanilladot_csvc_ROSE, anovadot_csvc_ROSE)
```

Sorting the rows in decreasing order order: 0.6·Sensitivity + 0.4·Specificity.

```{r}
cv_result <- cv_result[with(cv_result, order(0.6*cv_result[,2]+0.4*cv_result[,3], decreasing = TRUE)), ]
rownames(cv_result) <- NULL
cv_result
```

### Saving the results

Saving the results to the folder "1.3. KSVM".

```{r}
dir.create(file.path("../Results", "1. Cross_Validation", "1.1. KSVM"), recursive = TRUE,showWarnings = FALSE)

file_name <- paste0('../Results/1. Cross_Validation/1.1. KSVM/',target_name, '_KSVM_', csv)
write.csv(cv_result, file = file_name)
```
